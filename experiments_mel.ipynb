{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/coursework24/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from audiointerp.dataset.esc50 import ESC50dataset\n",
    "from audiointerp.model.cnn14 import TransferCnn14\n",
    "from audiointerp.fit import Trainer\n",
    "from audiointerp.processing.spectrogram import LogMelSTFTSpectrogram\n",
    "from audiointerp.interpretation.saliency import SaliencyInterpreter\n",
    "from audiointerp.interpretation.gradcam import GradCAMInterpreter\n",
    "from audiointerp.interpretation.shap import SHAPInterpreter\n",
    "from audiointerp.interpretation.lime import LIMEInterpreter\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T_audio\n",
    "import torchvision.transforms as T_vision\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "from audiointerp.predict import Predict\n",
    "from audiointerp.metrics import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/root/ESC50\"\n",
    "sr = 32000\n",
    "train_folds = [1, 2, 3]\n",
    "valid_folds = [4]\n",
    "test_folds = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 1024\n",
    "hop_length = 320\n",
    "win_length = 1024\n",
    "n_mels = 64\n",
    "f_min = 50\n",
    "f_max = 14000\n",
    "top_db = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = LogMelSTFTSpectrogram(\n",
    "    n_fft=n_fft, win_length=win_length, hop_length=hop_length,\n",
    "    sample_rate=sr, n_mels=n_mels, f_min=f_min, f_max=f_max, top_db=top_db,\n",
    "    return_phase=False, return_full_db=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ESC50dataset(root_dir=root_dir, sr=sr, folds=test_folds, normalize=\"peak\", feature_extractor=feature_extractor)\n",
    "test_loader_kwargs = {\"batch_size\": 32, \"shuffle\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "model_cls = TransferCnn14\n",
    "model_kwargs = {\"num_classes\": 50, \"num_bins\": 64}\n",
    "model_pretrain_weights_path = \"weights/Cnn14_mAP=0.431.pth\"\n",
    "\n",
    "optimizer_cls = optim.Adam\n",
    "optimizer_kwargs = {\"lr\": 1e-4}\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss\n",
    "use_mixup = False\n",
    "mixup_alpha = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "model_trainer = Trainer(\n",
    "    model_cls=model_cls,\n",
    "    train_data=None,\n",
    "    train_loader_kwargs=None,\n",
    "    criterion_cls=criterion_cls,\n",
    "    optimizer_cls=optimizer_cls,\n",
    "    model_kwargs=model_kwargs,\n",
    "    model_pretrain_weights_path=model_pretrain_weights_path,\n",
    "    optimizer_kwargs=optimizer_kwargs,\n",
    "    device=device,\n",
    "    valid_data=None,\n",
    "    valid_loader_kwargs=None,\n",
    "    test_data=test_data,\n",
    "    test_loader_kwargs=test_loader_kwargs,\n",
    "    use_mixup=use_mixup,\n",
    "    mixup_alpha=mixup_alpha\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer.model.load_state_dict(torch.load(\"logmel_cnn14.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2639, Test Acc: 0.9225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2638887568563223, 0.9225)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence_val = -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_background_folds = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_background(dataloader, num_samples_per_class=2, device=\"cpu\"):\n",
    "    from collections import defaultdict\n",
    "    class_to_samples = defaultdict(list)\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        for x, y in zip(batch_x, batch_y):\n",
    "            if len(class_to_samples[y.item()]) < num_samples_per_class:\n",
    "                class_to_samples[y.item()].append(x.unsqueeze(0))\n",
    "    \n",
    "    background_tensors = []\n",
    "    for class_label, tensor_list in class_to_samples.items():\n",
    "        background_tensors.extend(tensor_list)\n",
    "    \n",
    "    background = torch.cat(background_tensors, dim=0).to(device)\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_predict = LogMelSTFTSpectrogram(\n",
    "    n_fft=n_fft, win_length=win_length, hop_length=hop_length,\n",
    "    sample_rate=sr, n_mels=n_mels, f_min=f_min, f_max=f_max, top_db=top_db,\n",
    "    return_phase=True, return_full_db=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_predict = ESC50dataset(root_dir=root_dir, sr=sr, folds=test_folds, normalize=\"peak\")\n",
    "test_loader_predict = DataLoader(test_data_predict, batch_size=1, shuffle=False)\n",
    "train_data_shap = ESC50dataset(root_dir=root_dir, sr=sr, folds=shap_background_folds, normalize=\"peak\", feature_extractor=feature_extractor)\n",
    "train_loader_shap = DataLoader(train_data_shap, batch_size=100, shuffle=False)\n",
    "shap_background = get_balanced_background(train_loader_shap, num_samples_per_class=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_saliency = Predict(model, feature_extractor_predict, interp_method_cls=SaliencyInterpreter, interp_method_kwargs={}, device=device)\n",
    "predict_gradcam = Predict(model, feature_extractor_predict, interp_method_cls=GradCAMInterpreter, interp_method_kwargs={\"target_layers\": [model.base.conv_block6.conv2]}, device=device)\n",
    "predict_lime = Predict(model, feature_extractor_predict, interp_method_cls=LIMEInterpreter, interp_method_kwargs={\"num_samples\": 1000}, device=device)\n",
    "predict_shap = Predict(model, feature_extractor_predict, interp_method_cls=SHAPInterpreter, interp_method_kwargs={\"background_data\": shap_background}, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_saliency = predict_saliency.predict_set(test_loader_predict, 'saliency_clean.csv', compute_first=True,\n",
    "                                                silence_val=silence_val, model_type=\"logmel_cnn14\", save_dir=\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gradcam = predict_gradcam.predict_set(test_loader_predict, 'gradcam_clean.csv', compute_first=True,\n",
    "                                                silence_val=silence_val, model_type=\"logmel_cnn14\", save_dir=\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lime = predict_lime.predict_set(test_loader_predict, 'lime_clean.csv', compute_first=True,\n",
    "                                        silence_val=silence_val, model_type=\"logmel_cnn14\", save_dir=\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_shap = predict_shap.predict_set(test_loader_predict, 'shap_clean.csv', compute_first=True,\n",
    "                                        silence_val=silence_val, model_type=\"logmel_cnn14\", save_dir=\"results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
